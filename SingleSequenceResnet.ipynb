{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SingleSequenceResnet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PurabPatel555/SingleSequenceResnet/blob/main/SingleSequenceResnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20HR2bw99In9"
      },
      "source": [
        "#Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7k8AaYlhlCZ"
      },
      "source": [
        "#Paths\n",
        "TRAIN_PATH = '/content/drive/My Drive/Sparks/SPOT-1D-single/data/train/train.DSSP'\n",
        "VALID_PATH = '/content/drive/My Drive/Sparks/SPOT-1D-single/data/validation/validation.DSSP'\n",
        "CHECKPOINT_PATH = '/content/drive/My Drive/Sparks/SingleSequenceResnetCheckpoint'\n",
        "MODEL_PATH = '/content/drive/My Drive/Sparks/SingleSequenceResnetModel'\n",
        "TEST_PATH = '/content/drive/My Drive/Sparks/SPOT-1D-single/data/SPOT/SPOT.dssp'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csMIFoF-Bq_u"
      },
      "source": [
        "#Imports\n",
        "import os, sys\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import keras\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "from numpy import argmax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66mEdZMEFYD3"
      },
      "source": [
        "#Hyperparameters\n",
        "n_layers = 60\n",
        "n_filters = 60\n",
        "epochs = 100\n",
        "bs_train=1\n",
        "bs_valid=1\n",
        "train_mode = False #Changes the dropout behaviour in the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYzFQhbFighB"
      },
      "source": [
        "#Data exploration and loading\n",
        "\"\"\"\n",
        "The format of this data is:\n",
        ">\n",
        "sequence1\n",
        "structure1\n",
        ">\n",
        "sequence2\n",
        "structure2\n",
        ".\n",
        ".\n",
        ".\n",
        "\"\"\"\n",
        "f = open(TRAIN_PATH, \"r\")\n",
        "sequences = []\n",
        "structures = []\n",
        "while True:\n",
        "  line = f.readline()\n",
        "  if len(line) == 0:\n",
        "    break\n",
        "  if (line.find('>') != -1):\n",
        "    sequence = f.readline()\n",
        "    structure = f.readline()\n",
        "    sequences.append(sequence)\n",
        "    structures.append(structure)\n",
        "f2 = open(VALID_PATH, \"r\")\n",
        "sequences_valid = []\n",
        "structures_valid = []\n",
        "while True:\n",
        "  line = f2.readline()\n",
        "  if len(line) == 0:\n",
        "    break\n",
        "  if ((line).find('>') != -1):\n",
        "    sequence_valid = f2.readline()\n",
        "    structure_valid = f2.readline()\n",
        "    sequences_valid.append(sequence_valid)\n",
        "    structures_valid.append(structure_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SblYui_gnAFO"
      },
      "source": [
        "#Create a data generator to load data for training\n",
        "class DataGenerator(keras.utils.Sequence):\n",
        "    def __init__(self, sequences, structures, batch_size=1, shuffle=True):\n",
        "        self.batch_size = batch_size\n",
        "        self.structures = structures\n",
        "        self.sequences = sequences\n",
        "        self.shuffle=shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(len(self.structures) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        seq_data_formatted=[]\n",
        "        str_data_formatted=[]\n",
        "        aas = 'ARNDCEQGHILKMFPSTWYV'  #Amino acid symbols (input feature categories)\n",
        "        aa_to_int = dict((a, i) for i, a in enumerate(aas))\n",
        "        sss = 'GHITEBSCX' #Secondary structure options (output categories) \n",
        "        ss_to_int = dict((s, i) for i, s in enumerate(sss))\n",
        "        seq_data = [self.sequences[i][:-1] for i in indexes]\n",
        "        lengths = [len(sequence) for sequence in seq_data]  #Keep lengths of each sequence (for later padding purposes)\n",
        "        max_length = max(lengths) #Find the maximum length in the batch (for padding all other sequences to this length)\n",
        "        for seq in seq_data:\n",
        "          padding = max_length-len(seq) #Find the number of padded elements needed in the sequence\n",
        "          integer_encoded_seq = [aa_to_int[aa] for aa in seq] #Encode each amino acid as an integer\n",
        "          integer_encoded_seq = np.pad(integer_encoded_seq, (0,padding), 'constant', constant_values=20)  #Pad \n",
        "          seq_one_hot = np.eye(21)[integer_encoded_seq] #One-hot encoding\n",
        "          seq_data_formatted.append(seq_one_hot.astype('float32'))\n",
        "        X = np.asarray(seq_data_formatted)\n",
        "\n",
        "        str_data = [self.structures[i][:-1] for i in indexes]\n",
        "        for str_ in str_data:\n",
        "          padding = max_length-len(str_)  #Find the number of padded elements needed in the structure\n",
        "          integer_encoded_str = [ss_to_int[ss] for ss in str_]  #Encode each secondary structure element as an integer\n",
        "          integer_encoded_str = np.pad(integer_encoded_str, (0,padding), 'constant', constant_values=8) #Pad\n",
        "          str_one_hot = np.eye(9)[integer_encoded_str]  #One-hot encoding\n",
        "          str_one_hot = str_one_hot[:,:-1]\n",
        "          str_data_formatted.append(str_one_hot.astype('float32'))\n",
        "        y = np.asarray(str_data_formatted)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.indexes = np.arange(len(self.structures))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N31HDlJBYPrB"
      },
      "source": [
        "#Define the Resnet model using Keras Functional API\n",
        "def block(input, n_filters, kernel, dilation):  #Helper function to create blocks\n",
        "  output_res = input\n",
        "  output = tf.keras.layers.Conv1D(filters=n_filters, kernel_size=kernel, strides=1, padding='same', dilation_rate=dilation)(input)\n",
        "  output = tfa.layers.InstanceNormalization()(output)\n",
        "  output = tf.keras.layers.ELU(alpha=1.0)(output)\n",
        "  output = tf.keras.layers.Dropout(rate=0.15)(output, training=train_mode)\n",
        "  output = tf.keras.layers.Conv1D(filters=n_filters, kernel_size=kernel, strides=1, padding='same', dilation_rate=dilation)(output)\n",
        "  output = tf.keras.layers.ELU(alpha=1.0)(output_res+output)\n",
        "  return output\n",
        "\n",
        "inputs = tf.keras.Input(shape=(None,21), dtype=tf.float32)  #Input layer\n",
        "x = inputs\n",
        "x = tf.keras.layers.Conv1D(filters=n_filters, kernel_size=3, strides=1, padding='same')(x)\n",
        "x = output = tfa.layers.InstanceNormalization()(x)\n",
        "x = tf.keras.layers.ELU(alpha=1.0)(x)\n",
        "\n",
        "dilation = 1\n",
        "for i in range(n_layers):\n",
        "  x = block(input=x, n_filters=n_filters, kernel=3, dilation=dilation)\n",
        "  dilation *= 2\n",
        "  if(dilation == 16):\n",
        "    dilation = 1\n",
        "outputs = x\n",
        "outputs = tf.keras.layers.Conv1D(8, kernel_size=3, strides=1, padding='same')(x)\n",
        "outputs = tf.nn.softmax(outputs)  #Output layer\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZjdKEL9G7mA"
      },
      "source": [
        "#Masked loss\n",
        "\"\"\"\n",
        "'X' in the structure data corresponds to an unknown label at that position, which must be ignored in the loss\n",
        "\"\"\"\n",
        "def loss(mask_label):\n",
        "    mask_label = K.variable(mask_label)\n",
        "    def masked_cce(y_true, y_pred):\n",
        "        mask = K.all(K.equal(y_true, mask_label), axis=-1)\n",
        "        mask = tf.math.logical_not(mask)\n",
        "        y_true = tf.boolean_mask(y_true, mask)\n",
        "        y_pred = tf.boolean_mask(y_pred, mask)\n",
        "        loss = K.categorical_crossentropy(y_true, y_pred)\n",
        "        mask = K.cast(mask, K.floatx())\n",
        "        return K.sum(loss) / K.sum(mask)\n",
        "    return masked_cce\n",
        "\n",
        "masked_cce = loss(np.array([0, 0, 0, 0, 0, 0, 0, 0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2N5gJdJPSs_Y"
      },
      "source": [
        "#Q8 Accuracy (Masked accuracy)\n",
        "def get_accuracy(mask_label):\n",
        "  mask_label = K.variable(mask_label)\n",
        "  def accuracy_fun(y_true, y_pred):\n",
        "    mask = K.all(K.equal(y_true, mask_label), axis=-1)\n",
        "    mask = 1 - K.cast(mask, K.floatx())\n",
        "    m = tf.keras.metrics.CategoricalAccuracy()\n",
        "    m.reset_states()\n",
        "    m.update_state(y_true, y_pred, sample_weight=mask)\n",
        "    accuracy = m.result()\n",
        "    return accuracy\n",
        "  return accuracy_fun \n",
        "\n",
        "accuracy = get_accuracy(np.array([0, 0, 0, 0, 0, 0, 0, 0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKh9hnb8PV_q"
      },
      "source": [
        "#Compile the model\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=masked_cce,\n",
        "    metrics = [accuracy] #Comment this out when training to avoid eager mode slowdown\n",
        ")\n",
        "#model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JC1UQLClBks"
      },
      "source": [
        "#Initialize the data generators\n",
        "data_generator_train = DataGenerator(sequences=sequences, structures=structures, batch_size=bs_train)\n",
        "data_generator_valid = DataGenerator(sequences=sequences_valid, structures=structures_valid, batch_size=bs_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SS2VbVwo0hau"
      },
      "source": [
        "#Create an early stopping callback and a model checkpoint callback\n",
        "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=0)\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=CHECKPOINT_PATH,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKUCHYgTjjnp"
      },
      "source": [
        "#Load most recent checkpoint if applicable\n",
        "model.load_weights(CHECKPOINT_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brYYbThpVtNE"
      },
      "source": [
        "#Train the model\n",
        "model.fit(\n",
        "    x=data_generator_train,\n",
        "    epochs=epochs,\n",
        "    verbose=1,\n",
        "    validation_data = data_generator_valid,\n",
        "    callbacks = [early_stopping_callback, model_checkpoint_callback]\n",
        "    )\n",
        "\n",
        "#Save the model\n",
        "model.save(MODEL_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ehLs_vpwY8D"
      },
      "source": [
        "#Evaluation\n",
        "bs_test = 1\n",
        "tf.config.run_functions_eagerly(True)\n",
        "\n",
        "#Load test data\n",
        "f3 = open(TEST_PATH, \"r\")\n",
        "sequences_test = []\n",
        "structures_test = []\n",
        "while True:\n",
        "  line = f3.readline()\n",
        "  if len(line) == 0:\n",
        "    break\n",
        "  if ((line).find('>') != -1):\n",
        "    sequence_test = f3.readline()\n",
        "    structure_test = f3.readline()\n",
        "    sequences_test.append(sequence_test)\n",
        "    structures_test.append(structure_test)\n",
        "\n",
        "#Initialize test data generator\n",
        "data_generator_test = DataGenerator(sequences=sequences_test, structures=structures_test, batch_size=bs_test)\n",
        "\n",
        "#Load model weights\n",
        "model.load_weights(CHECKPOINT_PATH)\n",
        "\n",
        "#Evaluate the model\n",
        "results = model.evaluate(x=data_generator_test)\n",
        "print(\"test loss, test accuracy: \", results)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}